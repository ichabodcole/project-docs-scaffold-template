# Reports

This directory contains structured assessment and analysis reports that evaluate the current state of code, documentation, or systems. Reports are typically generated by automated processes or AI agents and serve as discovery mechanisms that can trigger new cycles of investigation and improvement.

---

## Purpose

The main purpose of reports is to **assess and synthesize current state** - providing structured findings about what exists, what's working, and what needs attention. Reports complete the documentation lifecycle by feeding discoveries back into it.

Reports are:

- **Retrospective assessments** - "Here's what we found when examining X"
- **Structured findings** - Organized analysis with actionable insights
- **Discovery mechanisms** - Triggering investigations, proposals, or direct action
- **Reference artifacts** - Persistent documents for scanning, sharing, and archiving

### Why Document Reports?

- **Trigger improvement cycles** - Findings spawn investigations and proposals
- **Provide evidence** - Ground decisions in concrete analysis rather than assumptions
- **Track trends over time** - Compare reports to see if things are improving or degrading
- **Enable knowledge sharing** - Make automated assessments accessible to the team
- **Create accountability** - Document what was found and what action was taken

Reports differ from other documentation:

- **Investigations** explore uncertainty ("Should we do X?"); reports assess current state ("Here's what X looks like")
- **Sessions** document implementation journeys; reports synthesize findings from analysis
- **Lessons Learned** capture specific problem-solution pairs; reports provide broader assessments

---

## The Documentation Cycle

Reports complete the documentation lifecycle by feeding back into it:

```
Report → Investigation → Proposal → Plan → Implementation (Sessions) → Report
```

**Examples:**

- Security audit report finds 5 vulnerability patterns → Investigation into auth refactoring
- Doc status report shows 8 stale proposals → Investigation into cleanup approach
- Code review report finds duplicate logic → Proposal for refactoring
- Performance report shows degradation → Investigation into root cause

When a report triggers follow-up work, cross-reference it:

```markdown
# Investigation: Auth Refactoring Approach

## Question

Based on findings in [Security Audit Report (2025-11-12)](../reports/2025-11-12-security-audit-report.md),
should we refactor our authentication handling to address identified vulnerability patterns?
```

---

## When to Create a Report

Create a report when:

- **Running automated assessments** - Security audits, code reviews, dependency checks
- **Evaluating documentation status** - Reviewing docs for staleness, completeness, organization
- **Analyzing system state** - Performance analysis, test coverage, architecture assessment
- **Synthesizing findings** - Aggregating insights from multiple sources into structured output
- **Need persistent reference** - Want to read, share, or archive the findings

**Key test:** Is this a structured assessment that evaluates current state and might trigger follow-up action?

---

## When NOT to Create a Report

- **Exploring uncertainty** - Use investigations to explore "should we?" questions
- **Planning implementation** - Use proposals and plans for forward-looking work
- **Documenting what happened** - Use sessions for implementation journeys
- **Capturing specific lessons** - Use lessons-learned for targeted problem-solution pairs
- **One-off command output** - If the findings are trivial or won't be referenced, keep them in command line

**Rule of thumb:** If the assessment provides value as a persistent artifact (for reference, archiving, or triggering follow-up work), make it a report. Otherwise, keep it ephemeral.

---

## File Naming

- `YYYY-MM-DD-report-type-topic.md`
- Examples:
  - `2025-11-12-doc-status-report.md`
  - `2025-11-12-security-audit-report.md`
  - `2025-11-15-code-review-feature-x-report.md`
  - `2025-11-20-dependency-audit-report.md`
  - `2025-12-01-test-coverage-report.md`

Date prefix enables:

- Chronological tracking of the same report type over time
- Easy comparison to see trends
- Natural archiving of outdated reports

---

## Template

A ready-to-use template is available: **[YYYY-MM-DD-TEMPLATE-report.md](./YYYY-MM-DD-TEMPLATE-report.md)**

The template provides structure for common report types while remaining flexible for different assessment needs.

### Core Sections

- **Metadata** (Date, Report Type, Status, Generated By) - Consistent tracking info
- **Executive Summary** - High-level findings and recommendations at a glance
- **Scope** - What was assessed and what was excluded
- **Findings** - Detailed assessment organized by category or severity
- **Recommendations** - Actionable next steps prioritized by impact
- **Follow-up Actions** - Specific tasks, investigations, or proposals spawned by this report

Adapt sections as needed - different report types may emphasize different elements.

---

## Report Types

### Documentation Status Reports

Assess the state of project documentation:

- Completeness (missing docs, coverage gaps)
- Currency (stale proposals, outdated architecture docs)
- Organization (naming conventions, linking, structure)
- Quality (clarity, actionability, usefulness)

**Common triggers:** Periodic review, onboarding friction, major project changes

### Code Review Reports

Evaluate code quality in a specific area:

- Code patterns and anti-patterns
- Duplication and refactoring opportunities
- Complexity and maintainability
- Test coverage and quality
- Documentation completeness

**Common triggers:** Feature branch review, pre-merge assessment, refactoring consideration

### Security Audit Reports

Identify security vulnerabilities and risks:

- Known vulnerabilities (dependency audits)
- Code patterns that introduce risk
- Authentication/authorization gaps
- Input validation issues
- Configuration security

**Common triggers:** Periodic security review, pre-release checks, dependency updates

### Performance Analysis Reports

Assess system performance characteristics:

- Bottlenecks and slow operations
- Resource usage patterns
- Scalability concerns
- Optimization opportunities

**Common triggers:** Performance degradation, capacity planning, optimization sprints

### Dependency Audit Reports

Evaluate project dependencies:

- Outdated packages
- Security vulnerabilities
- License compliance
- Unused dependencies
- Breaking change risks

**Common triggers:** Periodic maintenance, pre-release checks, security alerts

### Architecture Assessment Reports

Review system design and architecture:

- Alignment with intended patterns
- Consistency across modules
- Technical debt accumulation
- Integration patterns
- Evolution opportunities

**Common triggers:** Major feature planning, architecture review cycles, scaling concerns

---

## Tips

### Generating Reports

- **Be systematic** - Use checklists or frameworks to ensure consistent coverage
- **Prioritize findings** - Not all issues are equal; categorize by severity/impact
- **Provide context** - Include file references, line numbers, code snippets
- **Be actionable** - Don't just identify problems; suggest specific next steps
- **Quantify when possible** - "12 stale proposals" is better than "several old docs"

### Report Structure

- **Lead with executive summary** - Busy readers should get key findings immediately
- **Organize logically** - Group findings by category, severity, or system area
- **Link generously** - Reference files, commits, issues, related docs
- **Include timestamps** - Reports can become stale; date metadata is crucial
- **Tag appropriately** - Use tags/labels to make reports searchable

### Acting on Reports

- **Triage findings** - Not everything needs immediate action
- **Create follow-up tasks** - Capture next steps in investigations, proposals, or issue tracker
- **Cross-reference** - Link from follow-up work back to the originating report
- **Track trends** - Compare similar reports over time to measure progress

### Archiving Reports

Reports can become stale as systems evolve:

- **Archive superseded reports** - When a new report covers the same area
- **Keep for historical context** - Old reports can show progress or explain past decisions
- **Mark clearly** - Use status metadata to indicate "Archived" or "Superseded"
- **Link to successors** - Point from old reports to newer assessments

---

## Relationship to Other Documentation

- **Investigations** - Reports provide evidence that triggers investigations; investigations reference reports as context
- **Proposals** - Reports identify problems that proposals aim to solve; proposals reference reports as motivation
- **Plans** - Reports can trigger proposals that lead to plans
- **Sessions** - Implementation work may generate new reports (e.g., post-implementation code review)
- **Lessons Learned** - Reports may uncover patterns that become lessons learned
- **Architecture** - Architecture assessment reports evaluate existing architecture docs

---

## Frequently Asked Questions

### How is a report different from an investigation?

**Report:** "Here's what we found when assessing X" (retrospective, current-state)
**Investigation:** "Should we do something about X?" (exploratory, decision-focused)

Reports assess what exists; investigations explore whether to act.

### Can a report lead directly to a proposal without an investigation?

**Yes!** If the findings clearly indicate action is needed and the path forward is obvious, you can skip straight to a proposal. Use investigations when you need to explore options or gather more evidence first.

### Should reports be updated as systems change?

**Generally no.** Reports are point-in-time assessments. When significant changes occur, generate a new report rather than updating the old one. This creates a historical record showing trends over time.

**Exception:** If generating a report is interrupted or incomplete, it's fine to continue updating it until it's finalized. Once marked "Completed," treat it as immutable.

### How often should we generate reports?

**It depends on the report type and project needs:**

- **Security audits** - Monthly or quarterly, or triggered by dependency updates
- **Doc status** - Quarterly, or when documentation feels disorganized
- **Code reviews** - Per feature/branch, or periodic quality checks
- **Performance analysis** - When issues arise, or during optimization cycles

### Who generates reports?

Reports are typically generated by:

- **AI agents** - Automated assessments via custom commands or workflows
- **Automated tools** - Linters, security scanners, coverage tools
- **Developers** - Manual code reviews or architecture assessments

The "Generated By" metadata field captures this.

### What if a report finds nothing noteworthy?

That's a valid outcome! A report showing "all clear" is valuable information - it documents that an assessment was performed and nothing required attention. This prevents unnecessary re-checking and provides confidence in system health.

---

## Example Report Flow

### Example 1: Documentation Status → Investigation → Cleanup

1. **Report:** `2025-11-12-doc-status-report.md`
   - Finding: 12 proposals from 2024 with no corresponding plans
   - Finding: 5 architecture docs referencing deprecated patterns
   - Recommendation: Investigate relevance and update or archive

2. **Investigation:** `2025-11-13-stale-docs-investigation.md`
   - Question: Should we archive or update these old proposals?
   - Analysis: Reviews each proposal's current relevance
   - Recommendation: Archive 8, update 4

3. **Action:** Docs archived, relevant ones updated, patterns captured in lessons learned

### Example 2: Security Audit → Proposal → Plan

1. **Report:** `2025-11-15-security-audit-report.md`
   - Finding: 5 components with similar XSS vulnerability patterns
   - Finding: Input validation inconsistent across API endpoints
   - Recommendation: Standardize input sanitization approach

2. **Proposal:** `2025-11-16-input-sanitization-framework-proposal.md`
   - References security audit as motivation
   - Proposes unified sanitization library
   - Outlines implementation approach

3. **Plan:** `2025-11-20-input-sanitization-framework-plan.md`
   - Implementation roadmap based on proposal
   - Phases, acceptance criteria, timeline

### Example 3: Performance Report → Direct Action

1. **Report:** `2025-12-01-performance-analysis-report.md`
   - Finding: Document list rendering takes 850ms for 500 items
   - Root cause: Missing virtualization
   - Recommendation: Implement virtual scrolling (clear solution)

2. **Direct to Plan:** `2025-12-02-virtual-scrolling-implementation-plan.md`
   - No investigation needed - solution is obvious
   - References performance report as motivation
   - Outlines implementation approach

---

## Next Steps After Creating a Report

After generating a report, consider:

1. **Review findings** - Are there surprises? Patterns? Critical issues?
2. **Triage recommendations** - What needs immediate action vs. monitoring vs. ignoring?
3. **Create follow-up work:**
   - **Investigation** if unclear whether to act or need more analysis
   - **Proposal** if action is clearly warranted and you need to plan the solution
   - **Issues/tasks** for small, straightforward fixes
   - **Nothing** if findings show the system is healthy
4. **Share the report** - Ensure relevant team members are aware of findings
5. **Archive when superseded** - Mark the report as archived when a newer assessment is available
